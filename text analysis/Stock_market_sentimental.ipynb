{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from urllib.request import urlopen, Request\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import nltk\r\n",
    "import textblob\r\n",
    "from textblob import TextBlob\r\n",
    "import re\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "import seaborn as sns\r\n",
    "from wordcloud import WordCloud\r\n",
    "# nltk\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "# sklearn\r\n",
    "from sklearn.svm import LinearSVC\r\n",
    "from sklearn.naive_bayes import BernoulliNB\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.metrics import confusion_matrix, classification_report\r\n",
    "import stop_words\r\n",
    "\r\n",
    "\r\n",
    "nltk.downloader.download('vader_lexicon')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "finviz_url = 'https://finviz.com/quote.ashx?t='\r\n",
    "Companies = ['AMZN', 'GOOG', 'FB','TWTR'] # stock companies\r\n",
    "\r\n",
    "news_tables = {} # Declare empty dictionary to store results from finviz"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for company in Companies:\r\n",
    "    url = finviz_url + company # so it's going to loop first and get the company and url and then page and then comments from people after it will go back and take another company like GOOG\r\n",
    "    req = Request(url=url, headers={'user-agent':'my-app'}) # Specify headers or else access will be denied\r\n",
    "    response = urlopen(req)\r\n",
    "    soup = BeautifulSoup(response,features='html.parser')\r\n",
    "    news_table = soup.find(id='news-table')\r\n",
    "    news_tables.update({company:news_table})\r\n",
    "\r\n",
    "\r\n",
    "print(news_tables)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parsed_data = []\r\n",
    "\r\n",
    "for company, news_table in news_tables.items(): # this will go through the keys we created above, company and texts\r\n",
    "    for row in news_table.find_all('tr'): # the text in in tr, inorder to get all text we need to write find_all if we say find we will get  only tr\r\n",
    "        comment = row.a.text\r\n",
    "        date_data = row.td.text.split(' ')\r\n",
    "        if len(date_data) == 1:\r\n",
    "            time = date_data[0]\r\n",
    "        else:\r\n",
    "            date = date_data[0]\r\n",
    "            time = date_data[1]\r\n",
    "        parsed_data.append([company,date,time,comment])\r\n",
    "\r\n",
    "        \r\n",
    "print(parsed_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "df = pd.DataFrame(parsed_data, columns=['Company','Date','Time','Comments'])\r\n",
    "# Analyse your text\r\n",
    "vader = SentimentIntensityAnalyzer()\r\n",
    "\r\n",
    "print(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = []\r\n",
    "# Declare variables for scores\r\n",
    "compound_list = []\r\n",
    "positive_list = []\r\n",
    "negative_list = []\r\n",
    "neutral_list = []\r\n",
    "for i in range(df['Comments'].shape[0]):\r\n",
    "#print(analyser.polarity_scores(sentiments_pd['text'][i]))\r\n",
    "    compound =vader.polarity_scores(df['Comments'][i])[\"compound\"]\r\n",
    "    pos = vader.polarity_scores(df['Comments'][i])[\"pos\"]\r\n",
    "    neu = vader.polarity_scores(df['Comments'][i])[\"neu\"]\r\n",
    "    neg = vader.polarity_scores(df['Comments'][i])[\"neg\"]\r\n",
    "    \r\n",
    "    scores.append({\"Compound\": compound,\r\n",
    "                       \"Positive\": pos,\r\n",
    "                       \"Negative\": neg,\r\n",
    "                       \"Neutral\": neu\r\n",
    "                  })"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sentiments_score = pd.DataFrame.from_dict(scores)\r\n",
    "df = df.join(sentiments_score)\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Collect the compound values for each news source\r\n",
    "score_table = df.pivot_table(index='Company',  values=\"Neutral\", aggfunc = np.mean)\r\n",
    "score_table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score_table.plot(kind='bar')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Collect the compound values for each news source\r\n",
    "score_table = df.pivot_table(index='Company',  values=\"Compound\", aggfunc = np.mean)\r\n",
    "score_table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#plotting \r\n",
    "score_table.plot(kind='bar')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Collect the negative values for each news source\r\n",
    "neg_score_table = df.pivot_table(index='Company',  values=\"Negative\", aggfunc = np.mean)\r\n",
    "neg_score_table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#plotting \r\n",
    "neg_score_table.plot(kind='bar')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(6,8))\r\n",
    "# Using groupby makes us to have one date entry\r\n",
    "mean_df = df.groupby(['Company','Date']).mean()\r\n",
    "#print(mean_df)\r\n",
    "\r\n",
    "# Allow us to have date as x-axis\r\n",
    "mean_df = mean_df.unstack()\r\n",
    "# Remove compound column\r\n",
    "mean_df = mean_df.xs('Compound', axis='columns').transpose()\r\n",
    "mean_df.plot(kind='bar')\r\n",
    "plt.show()\r\n",
    "#print(mean_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.Comments.str.split(expand=True).stack().value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['Comments']=df['Comments'].str.lower()\r\n",
    "df.tail()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\r\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\r\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\r\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\r\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\r\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\r\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\r\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\r\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\r\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\r\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\r\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\r\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\r\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\r\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "STOPWORDS = set(stopwordlist)\r\n",
    "def cleaning_stopwords(text):\r\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\r\n",
    "df['Comments'] = df['Comments'].apply(lambda text: cleaning_stopwords(text))\r\n",
    "df['Comments'].head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import string\r\n",
    "english_punctuations = string.punctuation\r\n",
    "punctuations_list = english_punctuations\r\n",
    "def cleaning_punctuations(Comments):\r\n",
    "    translator = str.maketrans('', '', punctuations_list)\r\n",
    "    return Comments.translate(translator)\r\n",
    "df['Comments']= df['Comments'].apply(lambda x: cleaning_punctuations(x))\r\n",
    "df['Comments'].tail()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cleaning_repeating_char(Comments):\r\n",
    "    return re.sub(r'(.)1+', r'1', Comments)\r\n",
    "df['Comments'] = df['Comments'].apply(lambda x: cleaning_repeating_char(x))\r\n",
    "df['Comments'].tail()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cleaning_numbers(data):\r\n",
    "    return re.sub('[0-9]+', '', data)\r\n",
    "df['Comments'] = df['Comments'].apply(lambda x: cleaning_numbers(x))\r\n",
    "df['Comments'].tail()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get independent variables as X\r\n",
    "X = df.iloc[: , :-1].values\r\n",
    "Y = df.iloc[: , -1].values\r\n",
    "#print(Y)\r\n",
    "print(X)\r\n",
    "# print(df)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Encoding the independent variables\r\n",
    "\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "import numpy as np \r\n",
    "\r\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0,1,2,3])],remainder='passthrough') # Pass through will not encode the other columns. [3] represents index to encode\r\n",
    "X = ct.fit_transform(X).toarray()\r\n",
    "\r\n",
    "print(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Splitting data into Training and Test set\r\n",
    "# We need to train data so as to avoid over fitting and under fitting\r\n",
    "# We want the results of our Training and Test data to match\r\n",
    "# random_state controls the shuffling applied to the data before applying the split\r\n",
    "# test_size should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\r\n",
    "# print(x_train)\r\n",
    "# print(x_test)\r\n",
    "# print(y_train)\r\n",
    "# print(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Trainig the model\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.metrics import mean_absolute_error,max_error,explained_variance_score\r\n",
    "\r\n",
    "regressor = LinearRegression()\r\n",
    "regressor.fit(x_train,y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Testing \r\n",
    "pred = regressor.predict(x_test)\r\n",
    "np.set_printoptions(precision=2)\r\n",
    "joinedArrays = np.concatenate((pred.reshape(len(pred),1),y_test.reshape(len(y_test),1)),1) # Just 1 column. 0 = vertical axis and 1 = horizontal axis"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualising training results\r\n",
    "\r\n",
    "plt.scatter(x_train[:,0],y_train, color = 'red')\r\n",
    "plt.plot(x_train,regressor.predict(x_train),color = 'blue')\r\n",
    "plt.title('Training Results')\r\n",
    "plt.xlabel('Comments')\r\n",
    "plt.ylabel('Compound')\r\n",
    "plt.show() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualising test results\r\n",
    "\r\n",
    "plt.scatter(x_test[:,0],y_test, color = 'red')\r\n",
    "plt.plot(x_train,regressor.predict(x_train),color = 'blue')\r\n",
    "plt.title('Test Results')\r\n",
    "plt.xlabel('Comments')\r\n",
    "plt.ylabel('Compound')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import linear_model\r\n",
    "\r\n",
    "ols = linear_model.LinearRegression()\r\n",
    "model = ols.fit(X, Y)\r\n",
    "model.coef_ # The linear regression coefficient can be accessed in a form of class attribute with model.coef_\r\n",
    "model.intercept_ # The y-intercept can be accessed in a form of class attribute with model.intercept_\r\n",
    "model.score(X, Y) # How good was your model? You can evaluate your model performance in a form of R-squared, with model.score(X, y). X is the features, and y is the response variable used to fit the model.\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
    "import gensim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "coments_vectorizer = CountVectorizer(max_df = 0.90 , min_df = 2 , max_features = 1000,stop_words = 'english')\r\n",
    "comments = coments_vectorizer.fit_transform(df['Comments'])\r\n",
    "comments.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df=df.fillna(0) #replace all null values by 0\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "F_train, F_test, m_train, m_test = train_test_split(comments, df['Compound'],\r\n",
    "                                                    test_size=0.2, random_state=69)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"F_train_shape : \",F_train.shape)\r\n",
    "print(\"F_test_shape : \",F_test.shape)\r\n",
    "print(\"m_train_shape : \",m_train.shape)\r\n",
    "print(\"m_test_shape : \",m_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes Classifier\r\n",
    "\r\n",
    "model_naive = MultinomialNB().fit(F_train, m_train) \r\n",
    "predicted_naive = model_naive.predict(F_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix\r\n",
    "\r\n",
    "plt.figure(dpi=600)\r\n",
    "mat = confusion_matrix(m_test, predicted_naive)\r\n",
    "sns.heatmap(mat.T, annot=True, fmt='d', cbar=False)\r\n",
    "\r\n",
    "plt.title('Confusion Matrix for Naive Bayes')\r\n",
    "plt.xlabel('true label')\r\n",
    "plt.ylabel('predicted label')\r\n",
    "plt.savefig(\"confusion_matrix.png\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "score_naive = accuracy_score(predicted_naive, m_test)\r\n",
    "print(\"Accuracy with Naive-bayes: \",score_naive)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "247ab06e135bb35fa78c5eff31b2a9a0050dcb5fb773c2631d2a29ac689eeccb"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}